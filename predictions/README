By default, `classifier.py` will write your model predictions into this folder.
Before running `prepare_submit.py`, make sure that this directory has been populated!

GPT-2 Might Not Fully Benefit from Infini-Attention
GPT-2's default context length is already limited

The base GPT-2 model (like the 124M or 355M versions) has a maximum context length of 512 tokens.
Infini-attention is designed for models processing sequences in the hundreds of thousands to millions of tokens​.
GPT-2 typically does not need highly efficient memory mechanisms because its default input length is relatively short.
Infini-attention shines in ultra-long contexts

The main advantage of Infini-attention is allowing models to scale to 1M+ tokens while maintaining efficient memory use​.
For GPT-2, you may not reach a context size where standard attention becomes infeasible.
Computational Overhead vs. Benefit

Infini-attention requires additional operations like compressive memory updates and linear attention retrieval.
Given GPT-2's smaller size, these additions might introduce unnecessary complexity without significant performance gains.

Let's Justify the Experiment: Even though GPT-2 is small, there are ways to make Infini-attention relevant:

nfini-Attention Feasibility in GPT-2 (CS224N Project Alignment)

#1. Project Scope and Requirements

The CS224N Default Final Project requires implementing and fine-tuning GPT-2 on downstream NLP tasks, including:

Sentiment Analysis (Stanford Sentiment Treebank, CFIMDB) – Classifying text as positive, negative, or neutral.

Paraphrase Detection (Quora Question Pairs) – Identifying whether two sentences convey the same meaning.

Sonnet Generation (Shakespearean Sonnets) – Generating structured poetic text following rhyme and meter constraints.

While pretraining is not required, the project allows modifications to the attention mechanism to explore performance improvements.

#2. Motivation for Integrating Infini-Attention

GPT-2, in its standard form, has quadratic attention complexity, limiting its context length to 512 tokens. Infini-Attention provides an efficient memory mechanism to extend this context window, but its primary benefits apply to larger models handling sequences in the 100K-1M token range. Since GPT-2 is a relatively small model, integrating Infini-Attention may have diminishing returns due to:

Short default context window (512 tokens) – GPT-2 does not reach the computational limits where quadratic complexity becomes prohibitive.

Limited long-term dependencies in sentiment analysis and paraphrase detection – These tasks primarily operate on short sequences, reducing the impact of long-context memory.

Additional computational overhead – Infini-Attention introduces extra operations, which may not yield substantial gains at small scale.

However, we still see value in integrating Infini-Attention for experimental validation and potential efficiency improvements.

#3. Justification Within CS224N Guidelines

The project encourages architectural modifications that provide well-motivated improvements. Infini-Attention aligns with these guidelines by:

Extending GPT-2’s sequence length beyond 512 tokens to test whether increased memory capacity enhances performance.

Benchmarking against standard self-attention to quantify any efficiency or accuracy gains.

Evaluating impact across different NLP tasks:

Sentiment Analysis: Longer context retention enables better analysis of sentiment shifts within a passage, capturing nuanced emotional transitions in complex reviews and improving classification accuracy.

Paraphrase Detection: Improved attention over longer sequences allows the model to recognize subtle semantic equivalences or differences in extended question pairs, leading to better paraphrase identification.

Sonnet Generation: Retaining broader context ensures consistency in rhyme schemes, thematic continuity, and overall coherence across longer poetic compositions, improving generation quality.

#4. Hypotheses and Key Research Questions

We aim to answer the following:

Does Infini-Attention improve GPT-2’s ability to process long-form text efficiently?

How does memory usage and computational efficiency compare to the baseline GPT-2 model?

Are there specific NLP tasks where Infini-Attention offers measurable performance gains?

#5. Experimental Design

Baseline:

Implement standard GPT-2.

Fine-tune on sentiment analysis, paraphrase detection, and sonnet generation.

Infini-Attention Model:

Modify GPT-2’s attention mechanism to include compressive memory.

Fine-tune on the same tasks at varying sequence lengths (512, 1024, 2048 tokens).

Compare performance in terms of accuracy, computational cost, and memory efficiency.

#Potential extension

Application to Product Management & Roadmap Development
Product Managers frequently process large volumes of user feedback across different sources (Reddit, internal reports, customer service logs). Infini-Attention can help GPT-2 maintain context across multiple discussions, allowing PMs to:

Retain long-term insights from previous questions, responses, and evolving customer feedback.
Identify recurring themes and concerns in customer reviews.
Generate roadmap recommendations based on long-context sentiment shifts and product trends.
Implementation Approach
Context Window Expansion

Extend GPT-2's capacity using Infini-Attention to process thousands of tokens from past conversations.
Store historical PM questions and responses to maintain continuity when analyzing new product feedback.
Incremental Memory Updates

Use compressive memory updates so that the model "remembers" key insights from previous discussions.
Prioritize high-signal information (e.g., frequently mentioned issues) while discarding less relevant data.
Fine-Tuning on Real-world Data

Train on a dataset of Reddit reviews + internal customer service reports.
Evaluate how well GPT-2 with Infini-Attention tracks evolving customer pain points.